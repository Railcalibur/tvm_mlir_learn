{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import tvm.testing\n",
    "from tvm import te\n",
    "import numpy as np\n",
    "\n",
    "# 全局环境定义\n",
    "\n",
    "tgt_host = \"llvm\"\n",
    "# 如果启用了GPU，则将其更改为相应的GPU，例如：cuda、opencl、rocm\n",
    "tgt = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tvm.te.tensor.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "n = te.var(\"n\") # 定义符号变量 n\n",
    "A = te.placeholder((n,), name=\"A\")  # 定义占位符张量\n",
    "B = te.placeholder((n,), name=\"B\")\n",
    "C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")    # 计算规则\n",
    "print(type(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# 调度计算\n",
    "# 虽然上面的几行描述了计算规则，但是我们可以用很多方法来计算C，因为C可以在轴上用数据并行的方式来计算。TVM要求用户提供一个称为schedule的计算描述。\n",
    "# schedule是程序中变换计算循环的一组集合。在我们构造了schedule之后，默认情况下，schedule以串行方式按行的主要顺序计算C。\n",
    "#\n",
    "# .. code-block:: c\n",
    "#\n",
    "#   for (int i = 0; i < n; ++i) {\n",
    "#     C[i] = A[i] + B[i];\n",
    "#   }\n",
    "#\n",
    "s = te.create_schedule(C.op)\n",
    "\n",
    "######################################################################\n",
    "# 我们调用`te.create_schedule`来创建scheduler，然后使用split构造来拆分C的第一个轴，\n",
    "# 这将把原来的一个迭代轴拆分成两个迭代轴的乘积\n",
    "#\n",
    "# .. code-block:: c\n",
    "#\n",
    "#   for (int bx = 0; bx < ceil(n / 64); ++bx) {\n",
    "#     for (int tx = 0; tx < 64; ++tx) {\n",
    "#       int i = bx * 64 + tx;\n",
    "#       if (i < n) {\n",
    "#         C[i] = A[i] + B[i];\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "#\n",
    "bx, tx = s[C].split(C.op.axis[0], factor=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:16:11] /home/parker/workspace/tvm/src/te/schedule/schedule_lang.cc:184: Warning: Axis iter_var(i.outer, ) is already bind to another thread iter_var(blockIdx.x, , blockIdx.x)\n",
      "[14:16:11] /home/parker/workspace/tvm/src/te/schedule/schedule_lang.cc:184: Warning: Axis iter_var(i.inner, ) is already bind to another thread iter_var(threadIdx.x, , threadIdx.x)\n"
     ]
    }
   ],
   "source": [
    "# 最后，我们将迭代轴bx和tx绑定到GPU计算grid中的线程。这些是特定于GPU的构造，允许我们生成在GPU上运行的代码。\n",
    "#\n",
    "if tgt == \"cuda\" or tgt == \"rocm\" or tgt.startswith(\"opencl\"):\n",
    "    s[C].bind(bx, te.thread_axis(\"blockIdx.x\"))\n",
    "    s[C].bind(tx, te.thread_axis(\"threadIdx.x\"))\n",
    "\n",
    "######################################################################\n",
    "# Compilation\n",
    "# 上面我们已经完成了指定scheduler，接下来我们就可以将上面的所有代码编译成一个TVM的函数了。\n",
    "# 默认情况下，TVM会将其编译成一个类型擦除函数，可以直接从Python端调用。下面我们使用`tvm,build`来创建一个编译函数，\n",
    "# 编译函数接收scheduler，函数签名（包含输入输出）以及我们需要编译到的目标语言。编译`fadd`的结果是一个GPU设备函数\n",
    "# （如果涉及GPU）以及一个调用GPU函数的host端包装器。`fadd`是生成的主机包装函数，它在内部包含对生成的设备函数的引用。\n",
    "#\n",
    "Tar = tvm.target.Target(target=tgt, host=tgt_host)\n",
    "fadd = tvm.build(s, [A, B, C], Tar, name=\"myadd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39465863 0.49697846 0.29477423 ... 0.15670976 0.33382186 0.32561234] float32 cuda(0)\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# 编译后的TVM函数公开了一个简洁的C API，可以被任何语言调用。TVM在python中提供了一个最小\n",
    "# 的array API来帮助快速测试和原型开发。阵列API基于DLPack标准。这个array API基\n",
    "# 于https://github.com/dmlc/dlpack 标准。要运行这个函数，首先需要创建一个GPU context，\n",
    "# 然后使用`tvm.nd.array`将数据拷贝到GPU，再使用我们编译好的函数`fadd`来执行计算，最后\n",
    "# `asnumpy()`将GPU端的array拷贝回CPU使用numpy进行计算，最后比较两者的差距。这部分的代码如下：\n",
    "#\n",
    "device = tvm.device(tgt, 0)\n",
    "\n",
    "n = 1024\n",
    "a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), device)\n",
    "b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), device)\n",
    "c = tvm.nd.array(np.zeros(n, dtype=C.dtype), device)\n",
    "print(a, a.dtype, a.device)\n",
    "\n",
    "fadd(a, b, c)\n",
    "tvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----GPU code-----\n",
      "\n",
      "#ifdef _WIN32\n",
      "  using uint = unsigned int;\n",
      "  using uchar = unsigned char;\n",
      "  using ushort = unsigned short;\n",
      "  using int64_t = long long;\n",
      "  using uint64_t = unsigned long long;\n",
      "#else\n",
      "  #define uint unsigned int\n",
      "  #define uchar unsigned char\n",
      "  #define ushort unsigned short\n",
      "  #define int64_t long long\n",
      "  #define uint64_t unsigned long long\n",
      "#endif\n",
      "extern \"C\" __global__ void __launch_bounds__(64) myadd_kernel0(float* __restrict__ C, float* __restrict__ A, float* __restrict__ B, int n, int stride, int stride1, int stride2) {\n",
      "  if (((int)blockIdx.x) < (n >> 6)) {\n",
      "    C[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride2))] = (A[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride))] + B[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride1))]);\n",
      "  } else {\n",
      "    if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < n) {\n",
      "      C[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride2))] = (A[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride))] + B[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) * stride1))]);\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Inspect the Generated Code\n",
    "# --------------------------\n",
    "# You can inspect the generated code in TVM. The result of tvm.build\n",
    "# is a TVM Module. fadd is the host module that contains the host wrapper,\n",
    "# it also contains a device module for the CUDA (GPU) function.\n",
    "#\n",
    "# The following code fetches the device module and prints the content code.\n",
    "#\n",
    "\n",
    "if tgt == \"cuda\" or tgt == \"rocm\" or tgt.startswith(\"opencl\"):\n",
    "    dev_module = fadd.imported_modules[0]\n",
    "    print(\"-----GPU code-----\")\n",
    "    print(dev_module.get_source())\n",
    "else:\n",
    "    print(fadd.get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['myadd.so', 'myadd.ptx', 'myadd.tvm_meta.json', 'myadd.o']\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Save Compiled Module\n",
    "# --------------------\n",
    "# Besides runtime compilation, we can save the compiled modules into\n",
    "# a file and load them back later. This is called ahead of time compilation.\n",
    "#\n",
    "# The following code first performs the following steps:\n",
    "#\n",
    "# - It saves the compiled host module into an object file.\n",
    "# - Then it saves the device module into a ptx file.\n",
    "# - cc.create_shared calls a compiler (gcc) to create a shared library\n",
    "#\n",
    "from tvm.contrib import cc\n",
    "from tvm.contrib import utils\n",
    "\n",
    "temp = utils.tempdir()\n",
    "fadd.save(temp.relpath(\"myadd.o\"))\n",
    "if tgt == \"cuda\":\n",
    "    fadd.imported_modules[0].save(temp.relpath(\"myadd.ptx\"))\n",
    "if tgt == \"rocm\":\n",
    "    fadd.imported_modules[0].save(temp.relpath(\"myadd.hsaco\"))\n",
    "if tgt.startswith(\"opencl\"):\n",
    "    fadd.imported_modules[0].save(temp.relpath(\"myadd.cl\"))\n",
    "cc.create_shared(temp.relpath(\"myadd.so\"), [temp.relpath(\"myadd.o\")])\n",
    "print(temp.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Load Compiled Module\n",
    "# --------------------\n",
    "# We can load the compiled module from the file system and run the code.\n",
    "# The following code loads the host and device module separately and\n",
    "# re-links them together. We can verify that the newly loaded function works.\n",
    "#\n",
    "fadd1 = tvm.runtime.load_module(temp.relpath(\"myadd.so\"))\n",
    "if tgt == \"cuda\":\n",
    "    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.ptx\"))\n",
    "    fadd1.import_module(fadd1_dev)\n",
    "\n",
    "if tgt == \"rocm\":\n",
    "    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.hsaco\"))\n",
    "    fadd1.import_module(fadd1_dev)\n",
    "\n",
    "if tgt.startswith(\"opencl\"):\n",
    "    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.cl\"))\n",
    "    fadd1.import_module(fadd1_dev)\n",
    "\n",
    "fadd1(a, b, c)\n",
    "tvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Pack Everything into One Library\n",
    "# --------------------------------\n",
    "# In the above example, we store the device and host code separately.\n",
    "# TVM also supports export everything as one shared library.\n",
    "# Under the hood, we pack the device modules into binary blobs and link\n",
    "# them together with the host code.\n",
    "# Currently we support packing of Metal, OpenCL and CUDA modules.\n",
    "#\n",
    "fadd.export_library(temp.relpath(\"myadd_pack.so\"))\n",
    "fadd2 = tvm.runtime.load_module(temp.relpath(\"myadd_pack.so\"))\n",
    "fadd2(a, b, c)\n",
    "tvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
